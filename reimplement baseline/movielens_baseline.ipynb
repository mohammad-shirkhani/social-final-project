{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFwgLn8Zozok",
    "outputId": "c6312e35-973b-4d33-de9a-5c2cf500ebeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "!pip -q install scipy tqdm\n",
    "import torch, sys, types, textwrap, importlib\n",
    "print('CUDA device:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYZnF40xzmr7"
   },
   "outputs": [],
   "source": [
    "def register_module(name: str, src: str):\n",
    "    \"\"\"\n",
    "    Creates a ModuleType, executes the code, and registers it in sys.modules\n",
    "    under the desired name so that future imports work.\n",
    "    \"\"\"\n",
    "    module = types.ModuleType(name)\n",
    "    exec(textwrap.dedent(src), module.__dict__)\n",
    "    sys.modules[name] = module\n",
    "    # Create the parent package \n",
    "    if '.' in name:\n",
    "        parent, child = name.split('.', 1)\n",
    "        if parent not in sys.modules:\n",
    "            sys.modules[parent] = types.ModuleType(parent)\n",
    "        setattr(sys.modules[parent], child, module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-5GQLrS0tRK"
   },
   "outputs": [],
   "source": [
    "consts_src = '''\n",
    "\"\"\"DATASET_DIR = 'Digital_Music_dataset/'\n",
    "DATA_DIR = 'Digital_Music_data/'\n",
    "DATA_IX_DIR = 'Digital_Music_data_ix/'\n",
    "DATA_IX_MAPPING_DIR = 'Digital_Music_ix_mapping/'\n",
    "PATH_DATA_DIR = 'Digital_Music_path_data/'\n",
    "DATA_FILE = 'reviews_Digital_Music_5.json.gz'\"\"\"\n",
    "\n",
    "DATASET_DIR = 'MovieLens_100k_dataset/'\n",
    "DATA_DIR = 'MovieLens_100k_data/'\n",
    "DATA_IX_DIR = 'MovieLens_100k_data_ix/'\n",
    "DATA_IX_MAPPING_DIR = 'MovieLens_100k_ix_mapping/'\n",
    "PATH_DATA_DIR = 'MovieLens_100k_path_data/'\n",
    "DATA_FILE = 'u.data'\n",
    "\n",
    "\"\"\"DATASET_DIR = 'Douban_dataset/'\n",
    "DATA_DIR = 'Douban_data/'\n",
    "DATA_IX_DIR = 'Douban_data_ix/'\n",
    "DATA_IX_MAPPING_DIR = 'Douban_ix_mapping/'\n",
    "PATH_DATA_DIR = 'Douban_path_data/'\n",
    "DATA_FILE = 'training_test_dataset.mat'\"\"\"\n",
    "\n",
    "\"\"\"DATASET_DIR = 'Jester_dataset/'\n",
    "DATA_DIR = 'Jester_data/'\n",
    "DATA_IX_DIR = 'Jester_data_ix/'\n",
    "DATA_IX_MAPPING_DIR = 'Jester_ix_mapping/'\n",
    "PATH_DATA_DIR = 'Jester_path_data/'\n",
    "DATA_FILE = 'Jester.csv'\"\"\"\n",
    "\n",
    "USER_SIM_DICT = 'user_sim.dict'\n",
    "ITEM_SIM_DICT = 'item_sim.dict'\n",
    "USER_ITEM_DICT = 'user_item.dict'\n",
    "ITEM_USER_DICT = 'item_user.dict'\n",
    "ITEM_DIRECTOR_DICT = 'item_director.dict'\n",
    "DIRECTOR_ITEM_DICT = 'director_item.dict'\n",
    "ITEM_ACTOR_DICT = 'item_actor.dict'\n",
    "ACTOR_ITEM_DICT = 'actor_item.dict'\n",
    "TRAIN_USER_ITEM_DICT = 'train_user_item.dict'\n",
    "TRAIN_ITEM_USER_DICT = 'train_item_user.dict'\n",
    "VALID_USER_ITEM_DICT = 'valid_user_item.dict'\n",
    "VALID_ITEM_USER_DICT = 'valid_item_user.dict'\n",
    "TEST_USER_ITEM_DICT = 'test_user_item.dict'\n",
    "TEST_ITEM_USER_DICT = 'test_item_user.dict'\n",
    "USER_ITEM_1_DICT = 'user_item_1.dict'\n",
    "USER_ITEM_2_DICT = 'user_item_2.dict'\n",
    "USER_ITEM_3_DICT = 'user_item_3.dict'\n",
    "USER_ITEM_4_DICT = 'user_item_4.dict'\n",
    "USER_ITEM_5_DICT = 'user_item_5.dict'\n",
    "ITEM_USER_1_DICT = 'item_user_1.dict'\n",
    "ITEM_USER_2_DICT = 'item_user_2.dict'\n",
    "ITEM_USER_3_DICT = 'item_user_3.dict'\n",
    "ITEM_USER_4_DICT = 'item_user_4.dict'\n",
    "ITEM_USER_5_DICT = 'item_user_5.dict'\n",
    "TRAIN_PATH_FILE = 'train_path_file.txt'\n",
    "VALID_PATH_FILE = 'valid_path_file.txt'\n",
    "TEST_PATH_FILE = 'test_path_file.txt'\n",
    "\n",
    "TYPE_TO_IX = 'type_to_ix.dict'\n",
    "RELATION_TO_IX = 'relation_to_ix.dict'\n",
    "ENTITY_TO_IX = 'entity_to_ix.dict'\n",
    "IX_TO_TYPE = 'ix_to_type.dict'\n",
    "IX_TO_RELATION = 'ix_to_relation.dict'\n",
    "IX_TO_ENTITY = 'ix_to_entity.dict'\n",
    "\n",
    "PAD_TOKEN = '#PAD_TOKEN'\n",
    "USER_TYPE = 0\n",
    "ITEM_TYPE = 1\n",
    "PAD_TYPE = 2\n",
    "DIRECTOR_TYPE = 3\n",
    "ACTOR_TYPE = 4\n",
    "\n",
    "USER_ITEM_1_REL = 0\n",
    "USER_ITEM_2_REL = 1\n",
    "USER_ITEM_3_REL = 2\n",
    "USER_ITEM_4_REL = 3\n",
    "USER_ITEM_5_REL = 4\n",
    "ITEM_USER_1_REL = 5\n",
    "ITEM_USER_2_REL = 6\n",
    "ITEM_USER_3_REL = 7\n",
    "ITEM_USER_4_REL = 8\n",
    "ITEM_USER_5_REL = 9\n",
    "USER_SIM_REL = 10\n",
    "ITEM_SIM_REL = 11\n",
    "END_REL = 12\n",
    "PAD_REL = 13\n",
    "ITEM_DIRECTOR_REL = 14\n",
    "DIRECTOR_ITEM_REL = 15\n",
    "ITEM_ACTOR_REL = 16\n",
    "ACTOR_ITEM_REL = 17\n",
    "\n",
    "ENTITY_EMB_DIM = 128\n",
    "TYPE_EMB_DIM = 32\n",
    "REL_EMB_DIM = 32\n",
    "HIDDEN_DIM = 256\n",
    "ATTENTION_DIM = 128\n",
    "MAX_PATH_LEN = 5\n",
    "SAMPLES = 30\n",
    "# SAMPLES = 100\n",
    "'''\n",
    "register_module('constants.consts', consts_src)\n",
    "from constants import consts   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkAMAJ3j04L-"
   },
   "outputs": [],
   "source": [
    "format_src = '''\n",
    "import constants.consts as consts\n",
    "\n",
    "\"\"\"\n",
    "functions used for converting path data into format for the model\n",
    "\"\"\"\n",
    "\n",
    "def format_paths(paths, e_to_ix, t_to_ix, r_to_ix, sampels):\n",
    "    \"\"\"\n",
    "    Pads paths up to max path length, converting each path into tuple\n",
    "    of (padded_path, path length).\n",
    "    \"\"\"\n",
    "\n",
    "    new_paths = []\n",
    "    padding_path = pad_path([], e_to_ix, t_to_ix, r_to_ix, consts.MAX_PATH_LEN, consts.PAD_TOKEN)\n",
    "    for path in paths:\n",
    "        path_len = len(path)\n",
    "        pad_path(path, e_to_ix, t_to_ix, r_to_ix, consts.MAX_PATH_LEN, consts.PAD_TOKEN)\n",
    "        new_paths.append((path, path_len))\n",
    "    for i in range(sampels - len(paths)):\n",
    "        new_paths.append((padding_path, 1))\n",
    "    return new_paths\n",
    "\n",
    "\n",
    "def pad_path(seq, e_to_ix, t_to_ix, r_to_ix, max_len, padding_token):\n",
    "    \"\"\"\n",
    "    Pads paths up to max path length\n",
    "    \"\"\"\n",
    "    relation_padding = r_to_ix[padding_token]\n",
    "    type_padding = t_to_ix[padding_token]\n",
    "    entity_padding = e_to_ix[padding_token]\n",
    "\n",
    "    while len(seq) < max_len:\n",
    "        seq.append([entity_padding, type_padding, relation_padding])\n",
    "\n",
    "    return seq\n",
    "'''\n",
    "register_module('data.format', format_src)\n",
    "from data.format import format_paths, pad_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LekQVqCB1DTO"
   },
   "outputs": [],
   "source": [
    "path_extr_src = \"\"\"\n",
    "import sys\n",
    "from os import path\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "sys.path.append(path.dirname(path.dirname(path.abspath('./constants'))))\n",
    "import constants.consts as consts\n",
    "\n",
    "\n",
    "class PathState:\n",
    "    def __init__(self, path, length, entities):\n",
    "        self.path = path    # array of [entity, entity type, relation to next] triplets\n",
    "        self.length = length\n",
    "        self.entities = entities    # set to keep track of the entities alr in the path to avoid cycles\n",
    "\n",
    "\n",
    "def get_random_index(nums, max_length):\n",
    "    index_list = list(range(max_length))\n",
    "    random.shuffle(index_list)\n",
    "    return index_list[:nums]\n",
    "\n",
    "\n",
    "def find_paths_user_to_items(start_user, user_sim, item_sim, user_item_1, user_item_2, user_item_3, user_item_4,\n",
    "                             user_item_5, item_user_1,  item_user_2, item_user_3, item_user_4, item_user_5, max_length,\n",
    "                             sample_nums):\n",
    "    '''\n",
    "    Finds paths of max depth from a user to items\n",
    "    '''\n",
    "    item_to_paths = defaultdict(list)\n",
    "    stack = []\n",
    "    start = PathState([[start_user, consts.USER_TYPE, consts.END_REL]], 0, {start_user})\n",
    "    stack.append(start)\n",
    "    while len(stack) > 0:\n",
    "        front = stack.pop()\n",
    "        entity, type = front.path[-1][0], front.path[-1][1]\n",
    "        # add path to item_to_paths dict, just want paths of max_length rn since length in [2,3,4,5]\n",
    "        if type == consts.ITEM_TYPE and front.length == max_length:\n",
    "            item_to_paths[entity].append(front.path)\n",
    "\n",
    "        if front.length == max_length:\n",
    "            continue\n",
    "\n",
    "        if type == consts.USER_TYPE:\n",
    "            if entity in user_sim:\n",
    "                user_list = user_sim[entity]\n",
    "                index_list = get_random_index(sample_nums, len(user_list))\n",
    "                for index in index_list:\n",
    "                    user = user_list[index]\n",
    "                    if user not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.USER_SIM_REL\n",
    "                        new_path.append([user, consts.USER_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {user})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in user_item_1:\n",
    "                item_list = user_item_1[entity]\n",
    "                index_list = get_random_index(sample_nums, len(item_list))\n",
    "                for index in index_list:\n",
    "                    item = item_list[index]\n",
    "                    if item not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.USER_ITEM_1_REL\n",
    "                        new_path.append([item, consts.ITEM_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {item})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in user_item_2:\n",
    "                item_list = user_item_2[entity]\n",
    "                index_list = get_random_index(sample_nums, len(item_list))\n",
    "                for index in index_list:\n",
    "                    item = item_list[index]\n",
    "                    if item not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.USER_ITEM_2_REL\n",
    "                        new_path.append([item, consts.ITEM_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {item})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in user_item_3:\n",
    "                item_list = user_item_3[entity]\n",
    "                index_list = get_random_index(sample_nums, len(item_list))\n",
    "                for index in index_list:\n",
    "                    item = item_list[index]\n",
    "                    if item not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.USER_ITEM_3_REL\n",
    "                        new_path.append([item, consts.ITEM_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {item})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in user_item_4:\n",
    "                item_list = user_item_4[entity]\n",
    "                index_list = get_random_index(sample_nums, len(item_list))\n",
    "                for index in index_list:\n",
    "                    item = item_list[index]\n",
    "                    if item not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.USER_ITEM_4_REL\n",
    "                        new_path.append([item, consts.ITEM_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {item})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in user_item_5:\n",
    "                item_list = user_item_5[entity]\n",
    "                index_list = get_random_index(sample_nums, len(item_list))\n",
    "                for index in index_list:\n",
    "                    item = item_list[index]\n",
    "                    if item not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.USER_ITEM_5_REL\n",
    "                        new_path.append([item, consts.ITEM_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {item})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "        elif type == consts.ITEM_TYPE:\n",
    "            if entity in item_sim:\n",
    "                item_list = item_sim[entity]\n",
    "                index_list = get_random_index(sample_nums, len(item_list))\n",
    "                for index in index_list:\n",
    "                    item = item_list[index]\n",
    "                    if item not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.ITEM_SIM_REL\n",
    "                        new_path.append([item, consts.ITEM_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {item})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in item_user_1:\n",
    "                user_list = item_user_1[entity]\n",
    "                index_list = get_random_index(sample_nums, len(user_list))\n",
    "                for index in index_list:\n",
    "                    user = user_list[index]\n",
    "                    if user not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.ITEM_USER_1_REL\n",
    "                        new_path.append([user, consts.USER_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {user})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in item_user_2:\n",
    "                user_list = item_user_2[entity]\n",
    "                index_list = get_random_index(sample_nums, len(user_list))\n",
    "                for index in index_list:\n",
    "                    user = user_list[index]\n",
    "                    if user not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.ITEM_USER_2_REL\n",
    "                        new_path.append([user, consts.USER_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {user})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in item_user_3:\n",
    "                user_list = item_user_3[entity]\n",
    "                index_list = get_random_index(sample_nums, len(user_list))\n",
    "                for index in index_list:\n",
    "                    user = user_list[index]\n",
    "                    if user not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.ITEM_USER_3_REL\n",
    "                        new_path.append([user, consts.USER_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {user})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in item_user_4:\n",
    "                user_list = item_user_4[entity]\n",
    "                index_list = get_random_index(sample_nums, len(user_list))\n",
    "                for index in index_list:\n",
    "                    user = user_list[index]\n",
    "                    if user not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.ITEM_USER_4_REL\n",
    "                        new_path.append([user, consts.USER_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {user})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "            if entity in item_user_5:\n",
    "                user_list = item_user_5[entity]\n",
    "                index_list = get_random_index(sample_nums, len(user_list))\n",
    "                for index in index_list:\n",
    "                    user = user_list[index]\n",
    "                    if user not in front.entities:\n",
    "                        new_path = copy.deepcopy(front.path)\n",
    "                        new_path[-1][2] = consts.ITEM_USER_5_REL\n",
    "                        new_path.append([user, consts.USER_TYPE, consts.END_REL])\n",
    "                        new_state = PathState(new_path, front.length + 1, front.entities | {user})\n",
    "                        stack.append(new_state)\n",
    "\n",
    "    return item_to_paths\n",
    "\"\"\"\n",
    "register_module('path_extraction', path_extr_src)\n",
    "from path_extraction import find_paths_user_to_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSQDSNDn1p9T"
   },
   "outputs": [],
   "source": [
    "prep_src = r'''\n",
    "# ---------- آغاز فایل اصلی ----------\n",
    "import pandas as pd, numpy as np, gzip, json, pickle, argparse, random, sys, torch\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from path_extraction import find_paths_user_to_items     # همان ماند\n",
    "from data.format import format_paths                     # ← بدون تغییر لازم است\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from os import path, mkdir\n",
    "sys.path.append(path.dirname(path.abspath('../constants')))\n",
    "import constants.consts as consts\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_file',\n",
    "                        default=consts.DATA_FILE,\n",
    "                        help='Path to the json.gz file containing rating information')\n",
    "    parser.add_argument('--rating_data_file',\n",
    "                        default='rating_data.csv',\n",
    "                        help='Path to the csv file containing rating data')\n",
    "    parser.add_argument('--rating_train_data_file',\n",
    "                        default='rating_train.csv',\n",
    "                        help='Path to the csv file containing training data')\n",
    "    parser.add_argument('--rating_valid_data_file',\n",
    "                        default='rating_valid.csv',\n",
    "                        help='Path to the csv file containing validating data')\n",
    "    parser.add_argument('--rating_test_data_file',\n",
    "                        default='rating_test.csv',\n",
    "                        help='Path to the csv file containing testing data')\n",
    "    parser.add_argument('--split_data',\n",
    "                        default=False,\n",
    "                        help='whether to split the data')\n",
    "    parser.add_argument('--alpha',\n",
    "                        type=float,\n",
    "                        default=0.3,\n",
    "                        help='alpha for constructing similarity')\n",
    "\n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "def read_rating_data(data_file):\n",
    "    rating_data = []\n",
    "    with gzip.open(data_file) as f:\n",
    "        for l in f:\n",
    "            rating_data.append(json.loads(l.strip()))\n",
    "    rating_data_df = pd.DataFrame(rating_data)\n",
    "    rating_data_df = rating_data_df[['reviewerID', 'asin', 'overall']]\n",
    "    rating_data_df.columns = ['user_id', 'item_id', 'ratings']\n",
    "\n",
    "    # save data\n",
    "    rating_data_df.to_csv(consts.DATASET_DIR + 'rating_data.csv', index=False)\n",
    "\n",
    "\n",
    "def train_valid_test_split(rating_data_file, dir, training_data_file, validating_data_file, testing_data_file):\n",
    "    with open(consts.DATASET_DIR + rating_data_file, 'r', encoding='utf8') as fp:\n",
    "        data = pd.read_csv(fp)\n",
    "\n",
    "    # data split\n",
    "    valid_test = np.random.choice(len(data), size=int(0.2 * len(data)), replace=False)\n",
    "    valid_test_idx = np.zeros(len(data), dtype=bool)\n",
    "    valid_test_idx[valid_test] = True\n",
    "    rating_valid_test = data[valid_test_idx]\n",
    "    rating_train = data[~valid_test_idx]\n",
    "\n",
    "    num_ratings_valid_test = rating_valid_test.shape[0]\n",
    "    test = np.random.choice(num_ratings_valid_test, size=int(0.50 * num_ratings_valid_test), replace=False)\n",
    "    test_idx = np.zeros(num_ratings_valid_test, dtype=bool)\n",
    "    test_idx[test] = True\n",
    "    rating_test = rating_valid_test[test_idx]\n",
    "    rating_valid = rating_valid_test[~test_idx]\n",
    "\n",
    "    print(\"The number of training ratings is %d\" % (len(rating_train)))\n",
    "    print(\"The number of validating ratings is %d\" % (len(rating_valid)))\n",
    "    print(\"The number of testing ratings is %d\" % (len(rating_test)))\n",
    "\n",
    "    # save data\n",
    "    rating_train.to_csv(dir + training_data_file, index=False)\n",
    "    rating_valid.to_csv(dir + validating_data_file, index=False)\n",
    "    rating_test.to_csv(dir + testing_data_file, index=False)\n",
    "\n",
    "\n",
    "def create_directory(dir):\n",
    "    print(\"Creating directory %s\" % dir)\n",
    "    try:\n",
    "        mkdir(dir)\n",
    "    except FileExistsError:\n",
    "        print(\"Directory already exists\")\n",
    "\n",
    "\n",
    "def relation_cons(data_file, training_data_file, validating_data_file, testing_data_file, alpha, export_dir):\n",
    "    \"\"\"\n",
    "    return: Write out python dictionaries for the edge of graph\n",
    "    \"\"\"\n",
    "\n",
    "    with open(data_file, 'r', encoding='utf8') as fp:\n",
    "        data = pd.read_csv(fp)\n",
    "    with open(training_data_file, 'r', encoding='utf8') as fp:\n",
    "        rating_train = pd.read_csv(fp)\n",
    "    with open(validating_data_file, 'r', encoding='utf8') as fp:\n",
    "        rating_valid = pd.read_csv(fp)\n",
    "    with open(testing_data_file, 'r', encoding='utf8') as fp:\n",
    "        rating_test = pd.read_csv(fp)\n",
    "\n",
    "    user_item_dict = data.set_index('user_id').groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "    item_user_dict = data.set_index('item_id').groupby('item_id')['user_id'].apply(list).to_dict()\n",
    "    train_user_item_dict = rating_train.set_index('user_id').groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "    train_item_user_dict = rating_train.set_index('item_id').groupby('item_id')['user_id'].apply(list).to_dict()\n",
    "    valid_user_item_dict = rating_valid.set_index('user_id').groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "    valid_item_user_dict = rating_valid.set_index('item_id').groupby('item_id')['user_id'].apply(list).to_dict()\n",
    "    test_user_item_dict = rating_test.set_index('user_id').groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "    test_item_user_dict = rating_test.set_index('item_id').groupby('item_id')['user_id'].apply(list).to_dict()\n",
    "    with open(export_dir + consts.USER_ITEM_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_item_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + consts.ITEM_USER_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_user_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + consts.TRAIN_USER_ITEM_DICT, 'wb') as handle:\n",
    "        pickle.dump(train_user_item_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + consts.TRAIN_ITEM_USER_DICT, 'wb') as handle:\n",
    "        pickle.dump(train_item_user_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + consts.VALID_USER_ITEM_DICT, 'wb') as handle:\n",
    "        pickle.dump(valid_user_item_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + consts.VALID_ITEM_USER_DICT, 'wb') as handle:\n",
    "        pickle.dump(valid_item_user_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + consts.TEST_USER_ITEM_DICT, 'wb') as handle:\n",
    "        pickle.dump(test_user_item_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + consts.TEST_ITEM_USER_DICT, 'wb') as handle:\n",
    "        pickle.dump(test_item_user_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # id_count\n",
    "    users_id_num = data[\"user_id\"].unique().shape[0]\n",
    "    items_id_num = data[\"item_id\"].unique().shape[0]\n",
    "    ratings_num = data[\"ratings\"].shape[0]\n",
    "    print(\"the number of users: \", users_id_num)\n",
    "    print(\"the number of items: \", items_id_num)\n",
    "    print(\"the number of ratings: \", ratings_num)\n",
    "\n",
    "    # from id to num\n",
    "    # this id just used to construct the rating matrix\n",
    "    user2id = dict((uid, i) for (i, uid) in enumerate(data[\"user_id\"].unique()))\n",
    "    item2id = dict((sid, i) for (i, sid) in enumerate(data[\"item_id\"].unique()))\n",
    "    id2user = dict((i, uid) for (i, uid) in enumerate(data[\"user_id\"].unique()))\n",
    "    id2item = dict((i, sid) for (i, sid) in enumerate(data[\"item_id\"].unique()))\n",
    "    user_id = list(map(lambda x: user2id[x], rating_train['user_id']))\n",
    "    item_id = list(map(lambda x: item2id[x], rating_train['item_id']))\n",
    "    rating_train['user_id'] = user_id\n",
    "    rating_train['item_id'] = item_id\n",
    "\n",
    "    # construct rating matrix\n",
    "    # this matrix is used to construct the similarity of user pairs and item pairs\n",
    "    rating_matrix_arr = np.zeros((users_id_num, items_id_num), dtype=float)\n",
    "    for i in range(len(rating_train)):\n",
    "        rating_matrix_arr[int(rating_train.iloc[i][0]), int(rating_train.iloc[i][1])] = rating_train.iloc[i][2]\n",
    "    rating_matrix = torch.tensor(rating_matrix_arr, dtype=torch.float)\n",
    "    # construct similarity\n",
    "    user_sim_dict = {}\n",
    "    user_sim_nums = 0\n",
    "    user_sim_matrix = torch.cov(rating_matrix) / torch.sqrt(torch.mm(torch.var(rating_matrix, 1).unsqueeze(1),\n",
    "                                                                     torch.var(rating_matrix, 1).unsqueeze(0))) >= alpha\n",
    "    user_sparse = csr_matrix(user_sim_matrix)\n",
    "    user_sim_pairs = user_sparse.todok().keys()\n",
    "    for pair in user_sim_pairs:\n",
    "        if pair[0] != pair[1]:\n",
    "            user_sim_nums += 1\n",
    "            user_i, user_j = id2user[pair[0]], id2user[pair[1]]\n",
    "            if user_i not in user_sim_dict:\n",
    "                user_sim_dict[user_i] = []\n",
    "            user_sim_dict[user_i].append(user_j)\n",
    "    print(\"user similar pair numbers: \", int(user_sim_nums/2))\n",
    "    with open(export_dir + consts.USER_SIM_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_sim_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    item_sim_dict = {}\n",
    "    item_sim_nums = 0\n",
    "    item_sim_matrix = torch.cov(rating_matrix.transpose(1, 0))/torch.sqrt(torch.mm(torch.var(rating_matrix, 0).unsqueeze(1),\n",
    "                                                                torch.var(rating_matrix, 0).unsqueeze(0))) >= alpha\n",
    "    item_sparse = csr_matrix(item_sim_matrix)\n",
    "    item_sim_pairs = item_sparse.todok().keys()\n",
    "    for pair in item_sim_pairs:\n",
    "        if pair[0] != pair[1]:\n",
    "            item_sim_nums += 1\n",
    "            item_i, item_j = id2item[pair[0]], id2item[pair[1]]\n",
    "            if item_i not in item_sim_dict:\n",
    "                item_sim_dict[item_i] = []\n",
    "            item_sim_dict[item_i].append(item_j)\n",
    "    print(\"item similar pair numbers: \", int(item_sim_nums/2))\n",
    "    with open(export_dir + consts.ITEM_SIM_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_sim_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def data_prep(data_file, export_dir):\n",
    "    \"\"\"\n",
    "    return: Write out python dictionaries for the edges of graph\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r', encoding='utf8') as fp:\n",
    "        data = pd.read_csv(fp)\n",
    "\n",
    "    # train_user_item_k.dict\n",
    "    # dict where key = a user, value = list of item be ranked k by this user\n",
    "    user_item_1_dict = data[data[\"ratings\"] == 1].set_index('user_id').groupby('user_id')['item_id'].apply(\n",
    "        list).to_dict()\n",
    "    user_item_2_dict = data[data[\"ratings\"] == 2].set_index('user_id').groupby('user_id')['item_id'].apply(\n",
    "        list).to_dict()\n",
    "    user_item_3_dict = data[data[\"ratings\"] == 3].set_index('user_id').groupby('user_id')['item_id'].apply(\n",
    "        list).to_dict()\n",
    "    user_item_4_dict = data[data[\"ratings\"] == 4].set_index('user_id').groupby('user_id')['item_id'].apply(\n",
    "        list).to_dict()\n",
    "    user_item_5_dict = data[data[\"ratings\"] == 5].set_index('user_id').groupby('user_id')['item_id'].apply(\n",
    "        list).to_dict()\n",
    "    with open(export_dir + 'train_' + consts.USER_ITEM_1_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_item_1_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'train_' + consts.USER_ITEM_2_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_item_2_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'train_' + consts.USER_ITEM_3_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_item_3_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'train_' + consts.USER_ITEM_4_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_item_4_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'train_' + consts.USER_ITEM_5_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_item_5_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # train_item_user_k.dict\n",
    "    # dict where key = a item, value = list of user rank k to this item\n",
    "    item_user_1_dict = data[data[\"ratings\"] == 1].set_index('item_id').groupby('item_id')['user_id'].apply(\n",
    "        list).to_dict()\n",
    "    item_user_2_dict = data[data[\"ratings\"] == 2].set_index('item_id').groupby('item_id')['user_id'].apply(\n",
    "        list).to_dict()\n",
    "    item_user_3_dict = data[data[\"ratings\"] == 3].set_index('item_id').groupby('item_id')['user_id'].apply(\n",
    "        list).to_dict()\n",
    "    item_user_4_dict = data[data[\"ratings\"] == 4].set_index('item_id').groupby('item_id')['user_id'].apply(\n",
    "        list).to_dict()\n",
    "    item_user_5_dict = data[data[\"ratings\"] == 5].set_index('item_id').groupby('item_id')['user_id'].apply(\n",
    "        list).to_dict()\n",
    "    with open(export_dir + 'train_' + consts.ITEM_USER_1_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_user_1_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'train_' + consts.ITEM_USER_2_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_user_2_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'train_' + consts.ITEM_USER_3_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_user_3_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'train_' + consts.ITEM_USER_4_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_user_4_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'train_' + consts.ITEM_USER_5_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_user_5_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def ix_mapping(data_file, mapping_export_dir):\n",
    "    pad_token = consts.PAD_TOKEN\n",
    "    type_to_ix = {'user': consts.USER_TYPE, 'item': consts.ITEM_TYPE, pad_token: consts.PAD_TYPE}\n",
    "    relation_to_ix = {'user_sim': consts.USER_SIM_REL, 'item_sim': consts.ITEM_SIM_REL,\n",
    "                      'user_item_1': consts.USER_ITEM_1_REL,\n",
    "                      'user_item_2': consts.USER_ITEM_2_REL, 'user_item_3': consts.USER_ITEM_3_REL,\n",
    "                      'user_item_4': consts.USER_ITEM_4_REL,\n",
    "                      'user_item_5': consts.USER_ITEM_5_REL, 'item_user_1': consts.ITEM_USER_1_REL,\n",
    "                      'item_user_2': consts.ITEM_USER_2_REL,\n",
    "                      'item_user_3': consts.ITEM_USER_3_REL, 'item_user_4': consts.ITEM_USER_4_REL,\n",
    "                      'item_user_5': consts.ITEM_USER_5_REL,\n",
    "                      '#END_RELATION': consts.END_REL, pad_token: consts.PAD_REL}\n",
    "\n",
    "    # entity vocab set is combination of users and items\n",
    "    with open(data_file, 'r', encoding='utf8') as fp:\n",
    "        data = pd.read_csv(fp)\n",
    "\n",
    "    users = set(data[\"user_id\"].unique())\n",
    "    items = set(data[\"item_id\"].unique())\n",
    "\n",
    "    # Id-ix mappings\n",
    "    entity_to_ix = {(user, consts.USER_TYPE): ix for ix, user in enumerate(users)}\n",
    "    entity_to_ix.update({(item, consts.ITEM_TYPE): ix + len(users) for ix, item in enumerate(items)})\n",
    "    entity_to_ix[pad_token] = len(entity_to_ix)\n",
    "\n",
    "    # Ix-id mappings\n",
    "    ix_to_type = {v: k for k, v in type_to_ix.items()}\n",
    "    ix_to_relation = {v: k for k, v in relation_to_ix.items()}\n",
    "    ix_to_entity = {v: k for k, v in entity_to_ix.items()}\n",
    "\n",
    "    # Export mappings\n",
    "    # eg. Musical_Instruments_ix_mapping/type_to_ix.dict\n",
    "    with open(mapping_export_dir + consts.TYPE_TO_IX, 'wb') as handle:\n",
    "        pickle.dump(type_to_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(mapping_export_dir + consts.RELATION_TO_IX, 'wb') as handle:\n",
    "        pickle.dump(relation_to_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(mapping_export_dir + consts.ENTITY_TO_IX, 'wb') as handle:\n",
    "        pickle.dump(entity_to_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(mapping_export_dir + consts.IX_TO_TYPE, 'wb') as handle:\n",
    "        pickle.dump(ix_to_type, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(mapping_export_dir + consts.IX_TO_RELATION, 'wb') as handle:\n",
    "        pickle.dump(ix_to_relation, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(mapping_export_dir + consts.IX_TO_ENTITY, 'wb') as handle:\n",
    "        pickle.dump(ix_to_entity, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def convert_to_ids(entity_to_ix, rel_dict, start_type, end_type):\n",
    "    new_rel = {}\n",
    "    for key, values in rel_dict.items():\n",
    "        key_id = entity_to_ix[(key, start_type)]\n",
    "        value_ids = []\n",
    "        for val in values:\n",
    "            value_ids.append(entity_to_ix[(val, end_type)])\n",
    "        new_rel[key_id] = value_ids\n",
    "    return new_rel\n",
    "\n",
    "\n",
    "def ix_update(import_dir, mapping_dir, export_dir):\n",
    "    with open(mapping_dir + consts.ENTITY_TO_IX, 'rb') as handle:\n",
    "        entity_to_ix = pickle.load(handle)\n",
    "    with open(import_dir + consts.USER_SIM_DICT, 'rb') as handle:\n",
    "        user_sim_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.ITEM_SIM_DICT, 'rb') as handle:\n",
    "        item_sim_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.USER_ITEM_DICT, 'rb') as handle:\n",
    "        user_item_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.ITEM_USER_DICT, 'rb') as handle:\n",
    "        item_user_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.TRAIN_USER_ITEM_DICT, 'rb') as handle:\n",
    "        train_user_item_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.TRAIN_ITEM_USER_DICT, 'rb') as handle:\n",
    "        train_item_user_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.VALID_USER_ITEM_DICT, 'rb') as handle:\n",
    "        valid_user_item_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.VALID_ITEM_USER_DICT, 'rb') as handle:\n",
    "        valid_item_user_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.TEST_USER_ITEM_DICT, 'rb') as handle:\n",
    "        test_user_item_dict = pickle.load(handle)\n",
    "    with open(import_dir + consts.TEST_ITEM_USER_DICT, 'rb') as handle:\n",
    "        test_item_user_dict = pickle.load(handle)\n",
    "\n",
    "    # mapping id to ix\n",
    "    user_sim_ix = convert_to_ids(entity_to_ix, user_sim_dict, consts.USER_TYPE, consts.USER_TYPE)\n",
    "    item_sim_ix = convert_to_ids(entity_to_ix, item_sim_dict, consts.ITEM_TYPE, consts.ITEM_TYPE)\n",
    "    user_item_ix = convert_to_ids(entity_to_ix, user_item_dict, consts.USER_TYPE, consts.ITEM_TYPE)\n",
    "    item_user_ix = convert_to_ids(entity_to_ix, item_user_dict, consts.ITEM_TYPE, consts.USER_TYPE)\n",
    "    train_user_item_ix = convert_to_ids(entity_to_ix, train_user_item_dict, consts.USER_TYPE, consts.ITEM_TYPE)\n",
    "    train_item_user_ix = convert_to_ids(entity_to_ix, train_item_user_dict, consts.ITEM_TYPE, consts.USER_TYPE)\n",
    "    valid_user_item_ix = convert_to_ids(entity_to_ix, valid_user_item_dict, consts.USER_TYPE, consts.ITEM_TYPE)\n",
    "    valid_item_user_ix = convert_to_ids(entity_to_ix, valid_item_user_dict, consts.ITEM_TYPE, consts.USER_TYPE)\n",
    "    test_user_item_ix = convert_to_ids(entity_to_ix, test_user_item_dict, consts.USER_TYPE, consts.ITEM_TYPE)\n",
    "    test_item_user_ix = convert_to_ids(entity_to_ix, test_item_user_dict, consts.ITEM_TYPE, consts.USER_TYPE)\n",
    "    with open(export_dir + 'ix_' + consts.USER_SIM_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_sim_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.ITEM_SIM_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_sim_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.USER_ITEM_DICT, 'wb') as handle:\n",
    "        pickle.dump(user_item_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.ITEM_USER_DICT, 'wb') as handle:\n",
    "        pickle.dump(item_user_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.TRAIN_USER_ITEM_DICT, 'wb') as handle:\n",
    "        pickle.dump(train_user_item_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.TRAIN_ITEM_USER_DICT, 'wb') as handle:\n",
    "        pickle.dump(train_item_user_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.VALID_USER_ITEM_DICT, 'wb') as handle:\n",
    "        pickle.dump(valid_user_item_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.VALID_ITEM_USER_DICT, 'wb') as handle:\n",
    "        pickle.dump(valid_item_user_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.TEST_USER_ITEM_DICT, 'wb') as handle:\n",
    "        pickle.dump(test_user_item_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(export_dir + 'ix_' + consts.TEST_ITEM_USER_DICT, 'wb') as handle:\n",
    "        pickle.dump(test_item_user_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    for k in range(5):\n",
    "        data_file = 'train_user_item_' + str(k + 1) + '.dict'\n",
    "        with open(import_dir + data_file, 'rb') as handle:\n",
    "            user_item_dict = pickle.load(handle)\n",
    "        user_item_ix = convert_to_ids(entity_to_ix, user_item_dict, consts.USER_TYPE, consts.ITEM_TYPE)\n",
    "        with open(export_dir + 'ix_' + data_file, 'wb') as handle:\n",
    "            pickle.dump(user_item_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    for k in range(5):\n",
    "        data_file = 'train_item_user_' + str(k + 1) + '.dict'\n",
    "        with open(import_dir + data_file, 'rb') as handle:\n",
    "            item_user_dict = pickle.load(handle)\n",
    "        item_user_ix = convert_to_ids(entity_to_ix, item_user_dict, consts.ITEM_TYPE, consts.USER_TYPE)\n",
    "        with open(export_dir + 'ix_' + data_file, 'wb') as handle:\n",
    "            pickle.dump(item_user_ix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def sample_paths(paths, samples):\n",
    "    index_list = list(range(len(paths)))\n",
    "    random.shuffle(index_list)\n",
    "    indices = index_list[:samples]\n",
    "    return [paths[i] for i in indices]\n",
    "\n",
    "\n",
    "def construct_paths(data_file, training_data_file, validating_data_file, testing_data_file, import_dir, path_dir,\n",
    "                    mapping_dir, samples):\n",
    "    \"\"\"\n",
    "    Constructs paths from the target user to the target item\n",
    "    \"\"\"\n",
    "    create_directory(path_dir)\n",
    "    train_path_file = open(path_dir + consts.TRAIN_PATH_FILE, 'w')\n",
    "    valid_path_file = open(path_dir + consts.VALID_PATH_FILE, 'w')\n",
    "    test_path_file = open(path_dir + consts.TEST_PATH_FILE, 'w')\n",
    "\n",
    "    # load data\n",
    "    with open(data_file, 'r', encoding='utf8') as handle:\n",
    "        rating_data = pd.read_csv(handle)\n",
    "    with open(training_data_file, 'r', encoding='utf8') as handle:\n",
    "        rating_train = pd.read_csv(handle)\n",
    "    with open(validating_data_file, 'r', encoding='utf8') as handle:\n",
    "        rating_valid = pd.read_csv(handle)\n",
    "    with open(testing_data_file, 'r', encoding='utf8') as handle:\n",
    "        rating_test = pd.read_csv(handle)\n",
    "    with open(mapping_dir + consts.ENTITY_TO_IX, 'rb') as handle:\n",
    "        entity_to_ix = pickle.load(handle)\n",
    "    with open(mapping_dir + consts.TYPE_TO_IX, 'rb') as handle:\n",
    "        type_to_ix = pickle.load(handle)\n",
    "    with open(mapping_dir + consts.RELATION_TO_IX, 'rb') as handle:\n",
    "        relation_to_ix = pickle.load(handle)\n",
    "    with open(mapping_dir + consts.IX_TO_ENTITY, 'rb') as handle:\n",
    "        ix_to_entity = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_' + consts.USER_SIM_DICT, 'rb') as handle:\n",
    "        user_sim = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_' + consts.ITEM_SIM_DICT, 'rb') as handle:\n",
    "        item_sim = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_' + consts.USER_ITEM_DICT, 'rb') as handle:\n",
    "        user_item = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.USER_ITEM_1_DICT, 'rb') as handle:\n",
    "        user_item_1 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.USER_ITEM_2_DICT, 'rb') as handle:\n",
    "        user_item_2 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.USER_ITEM_3_DICT, 'rb') as handle:\n",
    "        user_item_3 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.USER_ITEM_4_DICT, 'rb') as handle:\n",
    "        user_item_4 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.USER_ITEM_5_DICT, 'rb') as handle:\n",
    "        user_item_5 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.ITEM_USER_1_DICT, 'rb') as handle:\n",
    "        item_user_1 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.ITEM_USER_2_DICT, 'rb') as handle:\n",
    "        item_user_2 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.ITEM_USER_3_DICT, 'rb') as handle:\n",
    "        item_user_3 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.ITEM_USER_4_DICT, 'rb') as handle:\n",
    "        item_user_4 = pickle.load(handle)\n",
    "    with open(import_dir + 'ix_train_' + consts.ITEM_USER_5_DICT, 'rb') as handle:\n",
    "        item_user_5 = pickle.load(handle)\n",
    "\n",
    "    # trackers for statistics\n",
    "    train_paths_not_found = 0\n",
    "    valid_paths_not_found = 0\n",
    "    test_paths_not_found = 0\n",
    "    total_interactions = 0\n",
    "    avg_num_paths = 0\n",
    "\n",
    "    for user, items in tqdm(list(user_item.items())):\n",
    "        total_interactions += len(items)\n",
    "        item_to_paths = None\n",
    "\n",
    "        for item in items:\n",
    "            if item_to_paths is None:\n",
    "                item_to_paths = find_paths_user_to_items(user, user_sim, item_sim, user_item_1, user_item_2,\n",
    "                                                         user_item_3, user_item_4, user_item_5, item_user_1,\n",
    "                                                         item_user_2, item_user_3,item_user_4, item_user_5, 2, 20)\n",
    "                item_to_paths_len3 = find_paths_user_to_items(user, user_sim, item_sim, user_item_1, user_item_2,\n",
    "                                                              user_item_3, user_item_4, user_item_5, item_user_1,\n",
    "                                                              item_user_2, item_user_3, item_user_4, item_user_5, 3, 10)\n",
    "                item_to_paths_len4 = find_paths_user_to_items(user, user_sim, item_sim, user_item_1, user_item_2,\n",
    "                                                              user_item_3, user_item_4, user_item_5, item_user_1,\n",
    "                                                              item_user_2, item_user_3, item_user_4, item_user_5, 4, 5)\n",
    "                \"\"\"item_to_paths_len5 = find_paths_user_to_items(user, user_sim, item_sim, user_item_1, user_item_2,\n",
    "                                                              user_item_3, user_item_4, user_item_5, item_user_1,\n",
    "                                                              item_user_2, item_user_3, item_user_4, item_user_5, 5, 3)\"\"\"\n",
    "                for i in item_to_paths_len3.keys():\n",
    "                    item_to_paths[i].extend(item_to_paths_len3[i])\n",
    "                for i in item_to_paths_len4.keys():\n",
    "                    item_to_paths[i].extend(item_to_paths_len4[i])\n",
    "                \"\"\"for i in item_to_paths_len5.keys():\n",
    "                    item_to_paths[i].extend(item_to_paths_len5[i])\"\"\"\n",
    "\n",
    "            # add paths for interaction\n",
    "            item_paths = item_to_paths[item]\n",
    "            item_paths = sample_paths(item_paths, samples)\n",
    "            rating = float(rating_data.loc[rating_data.user_id == ix_to_entity[user][0]].loc[\n",
    "                               rating_data.item_id == ix_to_entity[item][0]].ratings.values[0])\n",
    "            if len(item_paths) > 0:\n",
    "                interaction = (format_paths(item_paths, entity_to_ix, type_to_ix, relation_to_ix, samples), user, item, len(item_paths), rating)\n",
    "                if ix_to_entity[item][0] in rating_train.loc[rating_train.user_id == ix_to_entity[user][0]][\"item_id\"].unique():\n",
    "                    train_path_file.write(repr(interaction))\n",
    "                    train_path_file.write(\"\\n\")\n",
    "                elif ix_to_entity[item][0] in rating_valid.loc[rating_valid.user_id == ix_to_entity[user][0]][\"item_id\"].unique():\n",
    "                    valid_path_file.write(repr(interaction))\n",
    "                    valid_path_file.write(\"\\n\")\n",
    "                elif ix_to_entity[item][0] in rating_test.loc[rating_test.user_id == ix_to_entity[user][0]][\"item_id\"].unique():\n",
    "                    test_path_file.write(repr(interaction))\n",
    "                    test_path_file.write(\"\\n\")\n",
    "                avg_num_paths += len(item_paths)\n",
    "            else:\n",
    "                padding_path = [[[user, consts.USER_TYPE, consts.PAD_REL], [item, consts.ITEM_TYPE, consts.END_REL]]]\n",
    "                interaction = (format_paths(padding_path, entity_to_ix, type_to_ix, relation_to_ix, samples), user, item, len(padding_path), rating)\n",
    "                if ix_to_entity[item][0] in rating_train.loc[rating_train.user_id == ix_to_entity[user][0]][\"item_id\"].unique():\n",
    "                    train_paths_not_found += 1\n",
    "                    train_path_file.write(repr(interaction))\n",
    "                    train_path_file.write(\"\\n\")\n",
    "                elif ix_to_entity[item][0] in rating_valid.loc[rating_valid.user_id == ix_to_entity[user][0]][\"item_id\"].unique():\n",
    "                    valid_paths_not_found += 1\n",
    "                    valid_path_file.write(repr(interaction))\n",
    "                    valid_path_file.write(\"\\n\")\n",
    "                elif ix_to_entity[item][0] in rating_test.loc[rating_test.user_id == ix_to_entity[user][0]][\"item_id\"].unique():\n",
    "                    test_paths_not_found += 1\n",
    "                    test_path_file.write(repr(interaction))\n",
    "                    test_path_file.write(\"\\n\")\n",
    "                continue\n",
    "\n",
    "    avg_num_paths = avg_num_paths / (\n",
    "                total_interactions - train_paths_not_found - valid_paths_not_found - test_paths_not_found)\n",
    "\n",
    "    print(\"number of paths attempted to find:\", total_interactions)\n",
    "    print(\"number of train paths not found:\", train_paths_not_found)\n",
    "    print(\"number of valid paths not found:\", valid_paths_not_found)\n",
    "    print(\"number of test paths not found:\", test_paths_not_found)\n",
    "    print(\"avg num paths per interaction:\", avg_num_paths)\n",
    "\n",
    "    train_path_file.close()\n",
    "    valid_path_file.close()\n",
    "    test_path_file.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Data preparation:\")\n",
    "    args = parse_args()\n",
    "    print(\"Forming knowledge graph...\")\n",
    "    create_directory(consts.DATA_DIR)\n",
    "\n",
    "    # train_valid_test_split\n",
    "    if args.split_data:\n",
    "        # read data\n",
    "        # read_rating_data(consts.DATASET_DIR + args.data_file)\n",
    "        train_valid_test_split(args.rating_data_file, consts.DATASET_DIR, args.rating_train_data_file,\n",
    "                               args.rating_valid_data_file, args.rating_test_data_file)\n",
    "\n",
    "    relation_cons(consts.DATASET_DIR + args.rating_data_file, consts.DATASET_DIR + args.rating_train_data_file,\n",
    "                  consts.DATASET_DIR + args.rating_valid_data_file, consts.DATASET_DIR + args.rating_test_data_file,\n",
    "                  args.alpha, consts.DATA_DIR)\n",
    "\n",
    "    data_prep(consts.DATASET_DIR + args.rating_train_data_file, consts.DATA_DIR)\n",
    "\n",
    "    print(\"Mapping ids to indices...\")\n",
    "    create_directory(consts.DATA_IX_DIR)\n",
    "    create_directory(consts.DATA_IX_MAPPING_DIR)\n",
    "    ix_mapping(consts.DATASET_DIR + args.rating_data_file, consts.DATA_IX_MAPPING_DIR)\n",
    "    ix_update(consts.DATA_DIR, consts.DATA_IX_MAPPING_DIR, consts.DATA_IX_DIR)\n",
    "\n",
    "    print(\"Constructing paths from user to item...\")\n",
    "    construct_paths(consts.DATASET_DIR + args.rating_data_file, consts.DATASET_DIR + args.rating_train_data_file,\n",
    "                    consts.DATASET_DIR + args.rating_valid_data_file, consts.DATASET_DIR + args.rating_test_data_file,\n",
    "                    consts.DATA_IX_DIR, consts.PATH_DATA_DIR, consts.DATA_IX_MAPPING_DIR, consts.SAMPLES)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "register_module('data_preparation', prep_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qXf02Qo3Pm_"
   },
   "outputs": [],
   "source": [
    "train_src = r'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import linecache\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class InteractionData(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that can either store all interaction data in memory or load it line\n",
    "    by line when needed\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_path_file, in_memory=True):\n",
    "        self.in_memory = in_memory\n",
    "        self.file = train_path_file\n",
    "        self.num_interactions = 0\n",
    "        self.interactions = []\n",
    "        if in_memory:\n",
    "            with open(self.file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    self.interactions.append(eval(line.rstrip(\"\\n\")))\n",
    "            self.num_interactions = len(self.interactions)\n",
    "        else:\n",
    "            with open(self.file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    self.num_interactions += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load the specific interaction either from memory or from file line\n",
    "        if self.in_memory:\n",
    "            return self.interactions[idx]\n",
    "        else:\n",
    "            line = linecache.getline(self.file, idx+1)\n",
    "            return eval(line.rstrip(\"\\n\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_interactions\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom dataloader collate function since we have tuples of lists of paths\n",
    "    \"\"\"\n",
    "\n",
    "    data = [line[0] for line in batch]\n",
    "    user = [line[1] for line in batch]\n",
    "    item = [line[2] for line in batch]\n",
    "    val_len = [line[3] for line in batch]\n",
    "    target = [line[4] for line in batch]\n",
    "    user = torch.LongTensor(user)\n",
    "    item = torch.LongTensor(item)\n",
    "    val_len = torch.LongTensor(val_len)\n",
    "    target = torch.Tensor(target)\n",
    "    return [data, user, item, val_len, target]\n",
    "\n",
    "\n",
    "def train(model, train_paths_file, valid_paths_file, batch_size, epochs, model_path, load_checkpoint, not_in_memory,\n",
    "          lr, l2_reg):\n",
    "    \"\"\"\n",
    "    -trains and outputs a model using the input data\n",
    "    -formatted_data is a list of path lists, each of which consists of tuples of\n",
    "    (path, label, path_length), where the path is padded to ensure same overall length\n",
    "    \"\"\"\n",
    "    model = model.cuda()\n",
    "    loss_function = nn.MSELoss(reduction='none')\n",
    "\n",
    "    # l2 regularization is tuned from {10−5 , 10−4 , 10−3 , 10−2 }\n",
    "    # Learning rate is found from {0.001, 0.002, 0.01, 0.02} with grid search\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg)\n",
    "\n",
    "    if load_checkpoint:\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # DataLoader used for batches\n",
    "    interaction_data_train = InteractionData(train_paths_file, in_memory=not not_in_memory)\n",
    "    train_loader = DataLoader(dataset=interaction_data_train, collate_fn=my_collate, batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "    interaction_data_valid = InteractionData(valid_paths_file, in_memory=not not_in_memory)\n",
    "    valid_loader = DataLoader(dataset=interaction_data_valid, collate_fn=my_collate, batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model, training_rmse, training_mae = training(model, optimizer, loss_function, train_loader, model_path)\n",
    "        validating_rmse, validating_mae = predict(model, loss_function, valid_loader, model_path)\n",
    "        print(\"Epoch: %d, Training_RMSE: %f, Training_MAE: %f, Validating_RMSE: %f, Validating_MAE: %f\"\n",
    "              % (epoch + 1, training_rmse, training_mae, validating_rmse, validating_mae))\n",
    "\n",
    "\n",
    "def training(model, optimizer, loss_function, data_loader, model_path):\n",
    "    rmse_metric = np.zeros(2)\n",
    "    mae_metric = np.zeros(2)\n",
    "    model.train()\n",
    "    for interaction_batch, users, items, val_lens, targets in data_loader:\n",
    "        # construct tensor of all paths in batch, tensor of all lengths, and tensor of interaction id\n",
    "        paths = []\n",
    "        lengths = []\n",
    "        for inter_id, interaction_paths in enumerate(interaction_batch):\n",
    "            for path, length in interaction_paths:\n",
    "                paths.append(path)\n",
    "                lengths.append(length)\n",
    "        paths = torch.tensor(paths, dtype=torch.long, device='cuda')\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long, device='cuda')\n",
    "\n",
    "        model.zero_grad()\n",
    "        prediction_scores = model(paths, lengths, users.cuda(), items.cuda(),\n",
    "                                  val_lens.cuda(), is_training=True).cuda()\n",
    "\n",
    "        # Compute the loss, gradients, and update the parameters by calling .step()\n",
    "        loss = loss_function(prediction_scores, targets.cuda())\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        rmse_metric += (float(loss.sum()), len(targets))\n",
    "        mae_metric += (float(torch.sum(abs(prediction_scores - targets.cuda()))), len(targets))\n",
    "\n",
    "    mse = rmse_metric[0] / rmse_metric[1]\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mae_metric[0] / mae_metric[1]\n",
    "\n",
    "    # Save model to disk\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, model_path)\n",
    "\n",
    "    return model, rmse, mae\n",
    "\n",
    "\n",
    "def predict(model, loss_function, valid_loader, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    rmse_metric = np.zeros(2)\n",
    "    mae_metric = np.zeros(2)\n",
    "    for interaction_batch, users, items, val_lens, targets in valid_loader:\n",
    "        # construct tensor of all paths in batch, tensor of all lengths, and tensor of interaction id\n",
    "        paths = []\n",
    "        lengths = []\n",
    "        for inter_id, interaction_paths in enumerate(interaction_batch):\n",
    "            for path, length in interaction_paths:\n",
    "                paths.append(path)\n",
    "                lengths.append(length)\n",
    "        paths = torch.tensor(paths, dtype=torch.long, device='cuda')\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long, device='cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run the forward pass\n",
    "            prediction_scores = model(paths, lengths, users.cuda(), items.cuda(),\n",
    "                                      val_lens.cuda(), is_training=False).cuda()\n",
    "            # Compute the loss\n",
    "            loss = loss_function(prediction_scores, targets.cuda())\n",
    "        rmse_metric += (float(loss.sum()), len(targets))\n",
    "        mae_metric += (float(torch.sum(abs(prediction_scores - targets.cuda()))), len(targets))\n",
    "\n",
    "    mse = rmse_metric[0] / rmse_metric[1]\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mae_metric[0] / mae_metric[1]\n",
    "    return rmse, mae\n",
    "'''\n",
    "register_module('model.train', train_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COFIfiP53cv3"
   },
   "outputs": [],
   "source": [
    "comper_src = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import constants.consts as consts\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class COMPER(nn.Module):\n",
    "\n",
    "    def __init__(self, e_emb_dim, t_emb_dim, r_emb_dim, hidden_dim, attention_dim, e_vocab_size, t_vocab_size,\n",
    "                 r_vocab_size, miu):\n",
    "        super(COMPER, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.miu = miu\n",
    "\n",
    "        self.entity_embeddings = nn.Embedding(e_vocab_size, e_emb_dim)\n",
    "        self.type_embeddings = nn.Embedding(t_vocab_size, t_emb_dim)\n",
    "        self.rel_embeddings = nn.Embedding(r_vocab_size, r_emb_dim)\n",
    "\n",
    "        self.bias = nn.Embedding(e_vocab_size, 1)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(e_emb_dim + t_emb_dim + r_emb_dim, hidden_dim)\n",
    "\n",
    "        # The attention parameters\n",
    "        self.attention_W_q = nn.Linear(2 * e_emb_dim, attention_dim, bias=False)\n",
    "        self.attention_W_k = nn.Linear(hidden_dim, attention_dim, bias=False)\n",
    "        self.attention_W_b = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to label\n",
    "        self.linear1 = nn.Linear(hidden_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 1)\n",
    "        \"\"\"self.linear1 = nn.Linear(hidden_dim, 8)\n",
    "        self.linear2 = nn.Linear(8, 4)\n",
    "        self.linear3 = nn.Linear(4, 1)\"\"\"\n",
    "        # self.Dropout = nn.Dropout(0.5)\n",
    "\n",
    "    # def forward(self, paths, inter_ids, path_lengths, users, items, val_lens, batch_size, is_training):\n",
    "        # transpose, so entities 1st row, types 2nd row, and relations 3nd (these are dim 1 and 2 since batch is 0)\n",
    "        # this could just be the input if we want\n",
    "    def forward(self, paths, lengths, users, items, val_lens, is_training):\n",
    "        \"\"\"paths = paths.reshape(-1, consts.MAX_PATH_LEN, 3)\n",
    "        lengths = lengths.reshape(-1)\"\"\"\n",
    "\n",
    "        inter_ids, _ = torch.arange(0, paths.shape[0] / consts.SAMPLES, device='cuda').repeat(consts.SAMPLES).sort()\n",
    "\n",
    "        paths, inter_id, lengths, perm_idx = self.sort_batch(paths, inter_ids, lengths)\n",
    "        sample_indexs = (lengths > 1).nonzero().squeeze(-1)\n",
    "        drop_num = len(lengths) - len(sample_indexs)\n",
    "        paths = paths[sample_indexs]\n",
    "        lengths = lengths[sample_indexs]\n",
    "\n",
    "        t_paths = torch.transpose(paths, 1, 2)\n",
    "\n",
    "        # then concatenate embeddings, batch is index 0, so selecting along index 1\n",
    "        # right now we do fetch embedding for padding tokens, but that these aren't used\n",
    "        entity_embed = self.entity_embeddings(t_paths[:, 0, :])\n",
    "        type_embed = self.type_embeddings(t_paths[:, 1, :])\n",
    "        rel_embed = self.rel_embeddings(t_paths[:, 2, :])\n",
    "        triplet_embed = torch.cat((entity_embed, type_embed, rel_embed), 2)  # concatenates lengthwise\n",
    "\n",
    "        # we need dimensions to be input size x batch_size x embedding dim, so transpose first 2 dim\n",
    "        batch_sec_embed = torch.transpose(triplet_embed, 0, 1)\n",
    "\n",
    "        # pack padded sequences, so we don't do extra computation\n",
    "        packed_embed = nn.utils.rnn.pack_padded_sequence(batch_sec_embed, lengths)\n",
    "\n",
    "        # last_out is the output state before padding for each path, since we only want final output\n",
    "        # self.lstm.flatten_parameters()\n",
    "        packed_out, (last_out, _) = self.lstm(packed_embed)\n",
    "        path_embedding = last_out[-1]\n",
    "        path_embedding = torch.cat((path_embedding, torch.rand(drop_num, self.hidden_dim, device='cuda')), 0)\n",
    "        # Get attention pooling of path_embedding over interaction id groups\n",
    "        path_embedding = self.sort_path_embedding(path_embedding, perm_idx)\n",
    "        path_embedding = path_embedding.reshape(-1, consts.SAMPLES, self.hidden_dim)\n",
    "\n",
    "        users_embedding = self.entity_embeddings(users)\n",
    "        items_embedding = self.entity_embeddings(items)\n",
    "        queries = torch.cat((users_embedding, items_embedding), dim=1)\n",
    "        sub_graph_embeddings = self.Attention(queries, path_embedding, path_embedding, val_lens, is_training)\n",
    "\n",
    "        \"\"\"start = True\n",
    "        sub_graph_embeddings = torch.Tensor()\n",
    "        for i in range(batch_size):\n",
    "            # get ixs for this interaction\n",
    "            inter_ixs = (inter_ids == i).nonzero().squeeze(1)\n",
    "\n",
    "            # weighted pooled scores for this interaction\n",
    "            query = self.entity_embeddings(torch.tensor((users[i], items[i])).to(device)).reshape(-1)\n",
    "\n",
    "            sub_graph_embedding = self.Attention(path_embedding[inter_ixs], query, is_training)\n",
    "\n",
    "            if start:\n",
    "                # unsqueeze turns it into 2d tensor, so that we can concatenate along existing dim\n",
    "                sub_graph_embeddings = sub_graph_embedding\n",
    "                start = not start\n",
    "            else:\n",
    "                sub_graph_embeddings = torch.cat((sub_graph_embeddings, sub_graph_embedding), dim=0)\"\"\"\n",
    "\n",
    "        # pass through linear layers\n",
    "        \"\"\"layer_1 = self.Dropout(F.relu(self.linear1(sub_graph_embeddings)))\n",
    "        layer_2 = self.Dropout(F.relu(self.linear2(layer_1)))\n",
    "        predict_scores = self.linear3(layer_2).squeeze(1)\"\"\"\n",
    "\n",
    "        # predict_scores = self.linear2(self.Dropout(F.relu(self.linear1(sub_graph_embeddings))))\n",
    "        predict_scores = self.linear2(F.relu(self.linear1(sub_graph_embeddings)))\n",
    "        # predict_scores = self.linear1(sub_graph_embeddings).squeeze(1)\n",
    "\n",
    "        b_u = self.bias(users)\n",
    "        b_i = self.bias(items)\n",
    "        output = predict_scores + b_u + b_i + self.miu\n",
    "\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "\n",
    "    \"\"\"def Attention(self, paths_embedding, query, is_training):\n",
    "        features = self.attention_W_q(query) + self.attention_W_k(paths_embedding.unsqueeze(0))\n",
    "        features = torch.tanh(features)\n",
    "        weights = self.attention_W_b(features).squeeze(-1)\n",
    "        attention_weights = F.softmax(weights, dim=1)\n",
    "        sub_graph_embedding = torch.mm(attention_weights, paths_embedding).squeeze(1)\n",
    "        # sub_graph_embedding = torch.mean(paths_embedding, 0).unsqueeze(0)\n",
    "\n",
    "        return sub_graph_embedding\"\"\"\n",
    "\n",
    "\n",
    "    def Attention(self, queries, keys, values, val_lens, is_training):\n",
    "        queries, keys = self.attention_W_q(queries), self.attention_W_k(keys)\n",
    "        # queries = (batch_size,num_hidden) keys = (batch_size,num_keys,num_hidden)\n",
    "        features = queries.unsqueeze(1) + keys\n",
    "        features = torch.tanh(features)\n",
    "        scores = self.attention_W_b(features).squeeze(-1)  # (batch_size,num_keys)\n",
    "\n",
    "        # mask\n",
    "        mask = torch.arange((scores.shape[1]), dtype=torch.float32, device='cuda')[None, :] < val_lens[:, None]\n",
    "        scores[~mask] = -1e6\n",
    "        attention_weights = nn.functional.softmax(scores, dim=-1).unsqueeze(1)\n",
    "        fusion_result = torch.bmm(attention_weights, values).squeeze(1)\n",
    "        return fusion_result\n",
    "\n",
    "    def sort_path_embedding(self, path_embedding, indexes):\n",
    "        _, perm_idx = indexes.sort(0, descending=False)\n",
    "        seq_tensor = path_embedding[perm_idx]\n",
    "        return seq_tensor\n",
    "\n",
    "    def sort_batch(self, batch, indexes, lengths):\n",
    "        \"\"\"\n",
    "        sorts a batch of paths by path length, in decreasing order\n",
    "        \"\"\"\n",
    "        seq_lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        seq_tensor = batch[perm_idx]\n",
    "        indexes_tensor = indexes[perm_idx]\n",
    "        return seq_tensor.cuda(), indexes_tensor.cuda(), seq_lengths.cpu(), perm_idx.cuda()\n",
    "\n",
    "    \"\"\"def paths_split(self, interaction_batch):\n",
    "        # construct tensor of all paths in batch, tensor of all lengths, and tensor of interaction id\n",
    "        paths = []\n",
    "        lengths = []\n",
    "        inter_ids = []\n",
    "        for inter_id, interaction_paths in enumerate(interaction_batch):\n",
    "            for path, length in interaction_paths:\n",
    "                paths.append(path)\n",
    "                lengths.append(length)\n",
    "            inter_ids.extend([inter_id for i in range(len(interaction_paths))])\n",
    "\n",
    "        inter_ids = torch.tensor(inter_ids, dtype=torch.long)\n",
    "        paths = torch.tensor(paths, dtype=torch.long)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "        # sort based on path lengths, largest first, so that we can pack paths\n",
    "        s_path_batch, s_inter_ids, s_lengths = self.sort_batch(paths, inter_ids, lengths)\n",
    "        return s_path_batch.cuda(), s_inter_ids.cuda(), s_lengths.cpu()\"\"\"\n",
    "'''\n",
    "register_module('model.COMPER', comper_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cu46dW8V3kkm"
   },
   "outputs": [],
   "source": [
    "pred_src = r'''\n",
    "import model.train as _tr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from model.train import InteractionData, my_collate\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def test(model, test_paths_file, batch_size, model_path, not_in_memory):\n",
    "    model = model.to(device)\n",
    "    loss_function = nn.MSELoss(reduction='none')\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # DataLoader used for batches\n",
    "    interaction_data = InteractionData(test_paths_file, in_memory=not not_in_memory)\n",
    "    data_loader = DataLoader(dataset=interaction_data, collate_fn=my_collate, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.eval()\n",
    "    rmse_metric = np.zeros(2)\n",
    "    mae_metric = np.zeros(2)\n",
    "    for interaction_batch, users, items, val_lens, targets in data_loader:\n",
    "        # construct tensor of all paths in batch, tensor of all lengths, and tensor of interaction id\n",
    "        paths = []\n",
    "        lengths = []\n",
    "        for inter_id, interaction_paths in enumerate(interaction_batch):\n",
    "            for path, length in interaction_paths:\n",
    "                paths.append(path)\n",
    "                lengths.append(length)\n",
    "\n",
    "        paths = torch.tensor(paths, dtype=torch.long, device=device)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long, device=device)\n",
    "\n",
    "        # sort based on path lengths, largest first, so that we can pack paths\n",
    "        with torch.no_grad():\n",
    "            # Run the forward pass\n",
    "            prediction_scores = model(paths, lengths, users.cuda(), items.cuda(),\n",
    "                                      val_lens.cuda(), is_training=False).cuda()\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(prediction_scores, targets.cuda())\n",
    "        rmse_metric += (float(loss.sum()), len(targets))\n",
    "        mae_metric += (float(torch.sum(abs(prediction_scores - targets.cuda()))), len(targets))\n",
    "\n",
    "    mse = rmse_metric[0] / rmse_metric[1]\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mae_metric[0] / mae_metric[1]\n",
    "    return rmse, mae\n",
    "'''\n",
    "register_module('model.predictor', pred_src)\n",
    "\n",
    "import types, sys\n",
    "model_pkg = types.ModuleType('model')\n",
    "sys.modules['model'] = model_pkg\n",
    "model_pkg.InteractionData = sys.modules['model.train'].InteractionData\n",
    "model_pkg.my_collate  = sys.modules['model.train'].my_collate\n",
    "model_pkg.COMPER   = sys.modules['model.COMPER'].COMPER\n",
    "model_pkg.train    = sys.modules['model.train']\n",
    "model_pkg.test     = sys.modules['model.predictor'].test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIf7uR9n4omW"
   },
   "outputs": [],
   "source": [
    "main_src = r'''\n",
    "import pickle\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import constants.consts as consts\n",
    "from model import COMPER, train, test\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train',\n",
    "                        default=True,\n",
    "                        action='store_true',\n",
    "                        help='whether to train the model')\n",
    "    parser.add_argument('--eval',\n",
    "                        default=True,\n",
    "                        action='store_true',\n",
    "                        help='whether to evaluate the model')\n",
    "    parser.add_argument('--model',\n",
    "                        type=str,\n",
    "                        default='model.pt',\n",
    "                        help='name to save or load model from')\n",
    "    parser.add_argument('--load_checkpoint',\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help='whether to load the current model state before training ')\n",
    "    parser.add_argument('-e', '--epochs',\n",
    "                        type=int,\n",
    "                        default=5,\n",
    "                        help='number of epochs for training model')\n",
    "    parser.add_argument('-b', '--batch_size',\n",
    "                        type=int,\n",
    "                        default=256,\n",
    "                        help='batch_size')\n",
    "    parser.add_argument('--not_in_memory',\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help='denotes that the path data does not fit in memory')\n",
    "    parser.add_argument('--lr',\n",
    "                        type=float,\n",
    "                        default=0.01,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--l2_reg',\n",
    "                        type=float,\n",
    "                        default=0.01,\n",
    "                        help='l2 regularization coefficent')\n",
    "    parser.add_argument('--np_baseline',\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help='Run the model with the number of path baseline if True')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def load_string_to_ix_dicts():\n",
    "    \"\"\"\n",
    "    Loads the dictionaries mapping entity, relation, and type to id\n",
    "    \"\"\"\n",
    "    data_path = 'data/' + consts.DATA_IX_MAPPING_DIR\n",
    "\n",
    "    with open(data_path + consts.TYPE_TO_IX, 'rb') as handle:\n",
    "        type_to_ix = pickle.load(handle)\n",
    "    with open(data_path + consts.RELATION_TO_IX, 'rb') as handle:\n",
    "        relation_to_ix = pickle.load(handle)\n",
    "    with open(data_path + consts.ENTITY_TO_IX, 'rb') as handle:\n",
    "        entity_to_ix = pickle.load(handle)\n",
    "\n",
    "    return type_to_ix, relation_to_ix, entity_to_ix\n",
    "\n",
    "\n",
    "def get_miu(data_file):\n",
    "    with open(data_file, 'r', encoding='utf8') as fp:\n",
    "        training_data = pd.read_csv(fp)\n",
    "    miu = np.mean(training_data.iloc[:, 2])\n",
    "    return miu\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for model testing and training\n",
    "    \"\"\"\n",
    "    print(\"Main Loaded\")\n",
    "    random.seed(1000)\n",
    "    args = parse_args()\n",
    "    model_path = \"model/\" + args.model\n",
    "\n",
    "    t_to_ix, r_to_ix, e_to_ix = load_string_to_ix_dicts()\n",
    "\n",
    "    miu = get_miu('data/' + consts.DATASET_DIR + 'rating_train.csv')\n",
    "    model = COMPER(consts.ENTITY_EMB_DIM, consts.TYPE_EMB_DIM, consts.REL_EMB_DIM, consts.HIDDEN_DIM,\n",
    "                   consts.ATTENTION_DIM, len(e_to_ix), len(t_to_ix), len(r_to_ix), miu)\n",
    "\n",
    "    if args.train:\n",
    "        print(\"Training Starting\")\n",
    "\n",
    "        # 初始化\n",
    "        for m in model.children():\n",
    "            if isinstance(m, (nn.Embedding, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "        # load paths from disk\n",
    "        train_paths_file = 'data/' + consts.PATH_DATA_DIR + consts.TRAIN_PATH_FILE\n",
    "        valid_paths_file = 'data/' + consts.PATH_DATA_DIR + consts.VALID_PATH_FILE\n",
    "        train(model, train_paths_file, valid_paths_file, args.batch_size, args.epochs, model_path,\n",
    "              args.load_checkpoint, args.not_in_memory, args.lr, args.l2_reg)\n",
    "\n",
    "    if args.eval:\n",
    "        print(\"Evaluation Starting\")\n",
    "\n",
    "        # load paths from disk\n",
    "        test_paths_file = 'data/' + consts.PATH_DATA_DIR + consts.TEST_PATH_FILE\n",
    "        rmse, mae = test(model, test_paths_file, args.batch_size, model_path, args.not_in_memory)\n",
    "        print(\"Testing_RMSE: %f, Testing_MAE: %f\" % (rmse, mae))\n",
    "        with open('result/main_result.txt', 'a') as fp:\n",
    "            fp.write('RMSE = %s, MAE = %s\\n' % (rmse, mae))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "register_module('main', main_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IWF2pHQ52S2",
    "outputId": "fa202fe4-3f85-4056-dacf-8069b1ec209a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MovieLens-100K …\n",
      "Extracting …\n",
      "Converting u.data  →  rating_data.csv …\n",
      " MovieLens-100K ready → MovieLens_100k_dataset\n"
     ]
    }
   ],
   "source": [
    "import pathlib, urllib.request, zipfile, pandas as pd, os, shutil\n",
    "\n",
    "root = pathlib.Path('.')\n",
    "ds_dir = root / 'MovieLens_100k_dataset'\n",
    "ds_dir.mkdir(exist_ok=True)\n",
    "\n",
    "zip_path = root / 'ml-100k.zip'\n",
    "if not zip_path.exists():\n",
    "    print('Downloading MovieLens-100K …')\n",
    "    urllib.request.urlretrieve('https://files.grouplens.org/datasets/movielens/ml-100k.zip', zip_path)\n",
    "\n",
    "if not (ds_dir / 'u.data').exists():\n",
    "    print('Extracting …')\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(root)\n",
    "    shutil.move(root / 'ml-100k' / 'u.data', ds_dir / 'u.data')\n",
    "    print('Converting u.data  →  rating_data.csv …')\n",
    "    df = pd.read_csv(ds_dir / 'u.data', sep='\\t', header=None,\n",
    "                     names=['user_id','item_id','ratings','timestamp'])\n",
    "    df[['user_id','item_id','ratings']].to_csv(ds_dir / 'rating_data.csv', index=False)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    valid , test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "    train[['user_id','item_id','ratings']].to_csv(ds_dir/'rating_train.csv', index=False)\n",
    "    valid[['user_id','item_id','ratings']].to_csv(ds_dir/'rating_valid.csv', index=False)\n",
    "    test [['user_id','item_id','ratings']].to_csv(ds_dir/'rating_test.csv',  index=False)\n",
    "\n",
    "print(' MovieLens-100K ready →', ds_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZqyWE9j55ui",
    "outputId": "496fb0ae-f4a7-4e4a-c6c0-dbfd20c997a3"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation:\n",
      "Forming knowledge graph...\n",
      "Creating directory MovieLens_100k_data/\n",
      "the number of users:  943\n",
      "the number of items:  1682\n",
      "the number of ratings:  100000\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:155: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user similar pair numbers:  15516\n",
      "item similar pair numbers:  14797\n",
      "Mapping ids to indices...\n",
      "Creating directory MovieLens_100k_data_ix/\n",
      "Creating directory MovieLens_100k_ix_mapping/\n",
      "Constructing paths from user to item...\n",
      "Creating directory MovieLens_100k_path_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [1:43:41<00:00,  6.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of paths attempted to find: 100000\n",
      "number of train paths not found: 3\n",
      "number of valid paths not found: 15\n",
      "number of test paths not found: 18\n",
      "avg num paths per interaction: 29.73986635188668\n"
     ]
    }
   ],
   "source": [
    "import data_preparation\n",
    "data_preparation.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj90bcbNGq6A",
    "outputId": "ac4b5e7e-b197-4db7-a711-4fabd0ccf212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Loaded\n",
      "Training Starting\n",
      "Epoch: 1, Training_RMSE: 0.972978, Training_MAE: 0.773217, Validating_RMSE: 0.957811, Validating_MAE: 0.758035\n",
      "Epoch: 2, Training_RMSE: 0.935074, Training_MAE: 0.739534, Validating_RMSE: 0.940854, Validating_MAE: 0.736498\n",
      "Epoch: 3, Training_RMSE: 0.921025, Training_MAE: 0.726850, Validating_RMSE: 0.933321, Validating_MAE: 0.737486\n",
      "Epoch: 4, Training_RMSE: 0.911654, Training_MAE: 0.717986, Validating_RMSE: 0.930260, Validating_MAE: 0.734418\n",
      "Epoch: 5, Training_RMSE: 0.904607, Training_MAE: 0.712732, Validating_RMSE: 0.928204, Validating_MAE: 0.729147\n",
      "Evaluation Starting\n",
      "Testing_RMSE: 0.919606, Testing_MAE: 0.722576\n"
     ]
    }
   ],
   "source": [
    "import sys, os, shutil\n",
    "import constants.consts as consts\n",
    "\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs('result', exist_ok=True)\n",
    "\n",
    "import model\n",
    "train_module = sys.modules['model.train']\n",
    "train_func   = train_module.train\n",
    "model.train  = train_func\n",
    "if 'main' in sys.modules:\n",
    "    sys.modules['main'].train = train_func\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "for d in (\n",
    "    consts.DATASET_DIR.rstrip('/'),\n",
    "    consts.DATA_IX_MAPPING_DIR.rstrip('/'),\n",
    "    consts.PATH_DATA_DIR.rstrip('/')\n",
    "):\n",
    "    src = d\n",
    "    dst = os.path.join('data', d)\n",
    "    if os.path.exists(src) and not os.path.exists(dst):\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "sys.argv = [\n",
    "    'main',\n",
    "    '--train',\n",
    "    '--epochs', '5',\n",
    "    '-b', '256',\n",
    "    '--lr', '0.01',\n",
    "    '--l2_reg', '0.01'\n",
    "]\n",
    "\n",
    "import main\n",
    "main.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1qWDY4xMBXn",
    "outputId": "d25636ae-a2c6-438f-9790-b8f3c34cd3f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train split: 80000it [02:22, 560.53it/s]\n",
      "Processing val split: 10000it [00:18, 537.87it/s]\n",
      "Processing test split: 10000it [00:18, 548.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL files for MovieLens-100k saved in: /content/drive/MyDrive/COMPER_movie_lens_100k_meta_paths\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from constants import consts\n",
    "\n",
    "# 1) Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2) Define input and output paths\n",
    "LOCAL_PATH_DIR = os.path.join('data', consts.PATH_DATA_DIR)  # e.g., 'data/MovieLens_100k_path_data/'\n",
    "DEST_ROOT = '/content/drive/MyDrive/COMPER_movie_lens_100k_meta_paths'  # New directory for MovieLens-100k\n",
    "os.makedirs(DEST_ROOT, exist_ok=True)\n",
    "\n",
    "# 3) Define splits and their corresponding source files\n",
    "SPLITS = {\n",
    "    'train': consts.TRAIN_PATH_FILE,\n",
    "    'val': consts.VALID_PATH_FILE,\n",
    "    'test': consts.TEST_PATH_FILE\n",
    "}\n",
    "\n",
    "# 4) Function to convert formatted paths into JSON-friendly format\n",
    "def convert_paths(formatted_paths):\n",
    "    \"\"\"Convert [(padded_path, real_len), ...] to dicts for JSONL.\"\"\"\n",
    "    return [{'triplets': p, 'len': l} for p, l in formatted_paths]\n",
    "\n",
    "# 5) Process each split and write to JSONL with dataset-specific filenames\n",
    "for split, fname in SPLITS.items():\n",
    "    src = os.path.join(LOCAL_PATH_DIR, fname)\n",
    "    out_filename = f'{split}_movielens100k_meta_paths.jsonl'\n",
    "    dst = os.path.join(DEST_ROOT, out_filename)\n",
    "\n",
    "    with open(src, 'r') as fin, open(dst, 'w') as fout:\n",
    "        for line in tqdm(fin, desc=f'Processing {split} split'):\n",
    "            formatted_paths, user_idx, item_idx, _, rating = ast.literal_eval(line.strip())\n",
    "            record = {\n",
    "                'user_idx': int(user_idx),\n",
    "                'item_idx': int(item_idx),\n",
    "                'rating': float(rating),\n",
    "                'paths': convert_paths(formatted_paths)\n",
    "            }\n",
    "            fout.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print('JSONL files for MovieLens-100k saved in:', DEST_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhle05GIMYz4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
